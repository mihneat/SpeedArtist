{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6b53fbb-5cb0-4d1a-9a20-58612345850a",
   "metadata": {},
   "source": [
    "# Quick Draw Model - PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3286cfd-c6de-491f-8cb4-0124fed42654",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e42190-823f-44b6-b365-e527fcc9a6cd",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44ff9e5b-8c7b-42af-b762-7b2052ebfd96",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full power!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import json\n",
    "from datetime import datetime\n",
    "import functools\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Full power!\")\n",
    "    dev = torch.set_default_device(\"cuda\")\n",
    "else:\n",
    "    print(\"Regular power..\")\n",
    "    dev = torch.set_default_device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68710809-299a-4492-a0c0-43da5e7b684b",
   "metadata": {},
   "source": [
    "### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef25a92c-9181-4b3c-843d-88520c07ece3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.get_default_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ed76bd-4cd0-4044-b026-5009771eb375",
   "metadata": {},
   "source": [
    "## Manipulating The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373a13d0-632a-4481-a92d-9f6a34691283",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a7e525a-8859-4954-a502-2861db9d4971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some useful global variables\n",
    "\n",
    "classes = {}\n",
    "batch_size = 8\n",
    "dropout_rate = 0.3\n",
    "num_layers = 3\n",
    "num_nodes = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "276fd709-d169-499c-8913-fe9d958d8c0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "classes = {}\n",
    "\n",
    "def parseLine(ndjsonLine):\n",
    "  \"\"\"Parse an ndjson line and return ink (as np array) and classname.\"\"\"\n",
    "  sample = json.loads(ndjsonLine)\n",
    "  class_name = sample[\"word\"]\n",
    "  inkarray = sample[\"drawing\"]\n",
    "  stroke_lengths = [len(stroke[0]) for stroke in inkarray]\n",
    "  total_points = sum(stroke_lengths)\n",
    "  np_ink = np.zeros((total_points, 3), dtype=np.float32)\n",
    "  current_t = 0\n",
    "  for stroke in inkarray:\n",
    "    for i in [0, 1]:\n",
    "      np_ink[current_t:(current_t + len(stroke[0])), i] = stroke[i]\n",
    "    current_t += len(stroke[0])\n",
    "    np_ink[current_t - 1, 2] = 1  # stroke_end\n",
    "      \n",
    "  # Preprocessing.\n",
    "  # 1. Size normalization.\n",
    "  lower = np.min(np_ink[:, 0:2], axis=0)\n",
    "  upper = np.max(np_ink[:, 0:2], axis=0)\n",
    "  scale = upper - lower\n",
    "  scale[scale == 0] = 1\n",
    "  np_ink[:, 0:2] = (np_ink[:, 0:2] - lower) / scale\n",
    "    \n",
    "  # 2. Compute deltas.\n",
    "  np_ink[1:, 0:2] -= np_ink[0:-1, 0:2]\n",
    "  np_ink = np_ink[1:, :]\n",
    "  return torch.from_numpy(np_ink), class_name\n",
    "\n",
    "def readData(files, train_data, test_data, limit = -1):\n",
    "    # Clear the global variables\n",
    "    classes = {}\n",
    "    \n",
    "    filesToParse = files if limit < 0 else files[:limit]\n",
    "\n",
    "    currClassIndex = 0\n",
    "    classNameToIndex = {}\n",
    "    \n",
    "    cnt = 0\n",
    "    sampleCnt = 0\n",
    "    for filePath in filesToParse:        \n",
    "        with open(filePath) as file:\n",
    "            for line in file:\n",
    "                # sample = json.loads(line)\n",
    "                # className = sample[\"word\"]\n",
    "                features = {}\n",
    "                features[\"ink\"], features[\"className\"] = parseLine(line)\n",
    "\n",
    "                # Define the shape of the ink\n",
    "                features[\"shape\"] = features[\"ink\"].shape\n",
    "\n",
    "                # Index the class\n",
    "                if features[\"className\"] not in classNameToIndex:\n",
    "                    classNameToIndex[features[\"className\"]] = currClassIndex\n",
    "                    currClassIndex += 1\n",
    "\n",
    "                features[\"classIndex\"] = classNameToIndex[features[\"className\"]]\n",
    "\n",
    "                # Keep a class statistic\n",
    "                if features[\"className\"] not in classes:\n",
    "                    classes[features[\"className\"]] = 0\n",
    "\n",
    "                classes[features[\"className\"]] += 1\n",
    "\n",
    "                if sampleCnt % 11000 < 10000:\n",
    "                    train_data.append(features)\n",
    "                else:\n",
    "                    test_data.append(features)\n",
    "\n",
    "                sampleCnt += 1\n",
    "\n",
    "        cnt += 1\n",
    "\n",
    "        print(\"Finished parsing {0}/{1}: {2}\".format(cnt, len(files), filePath))\n",
    "\n",
    "    print(\"Finished parsing all the data!\")\n",
    "    return classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db4bd842-cfdd-45f8-a4ee-d2de5ba0eb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished parsing 1/16: datasets/full_simplified_airplane.ndjson\n",
      "Finished parsing 2/16: datasets/full_simplified_ant.ndjson\n",
      "Finished parsing 3/16: datasets/full_simplified_axe.ndjson\n",
      "Finished parsing 4/16: datasets/full_simplified_bed.ndjson\n",
      "Finished parsing all the data!\n",
      "\n",
      "{'airplane': 151623, 'ant': 124612, 'axe': 124122, 'bed': 113862}\n",
      "Train data len: 468219\n",
      "Test data len: 46000\n"
     ]
    }
   ],
   "source": [
    "qd_train_raw_data = []\n",
    "qd_test_raw_data = []\n",
    "\n",
    "root_dir = \"datasets\"\n",
    "\n",
    "dataFiles = [root_dir + \"/\" + f for f in listdir(root_dir) if f.endswith(\".ndjson\")]\n",
    "classes = readData(dataFiles, qd_train_raw_data, qd_test_raw_data, limit=4)\n",
    "\n",
    "print()\n",
    "print(classes)\n",
    "print(\"Train data len:\", len(qd_train_raw_data))\n",
    "print(\"Test data len:\", len(qd_test_raw_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4810f78b-e1e0-460d-b243-d0aeaa545b07",
   "metadata": {},
   "source": [
    "### Creating the Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7140d6dd-b651-4fe5-a650-090d3351af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickDrawDataset(Dataset):\n",
    "    \"\"\"Quick, Draw! data subset.\"\"\"\n",
    "\n",
    "    def __init__(self, data, classes, train):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            data (list): List of all the parsed data with the readData() function.\n",
    "            classes (dict): Dictionary with all the classes and how many of each there are.\n",
    "            train (bool): Says if the dataset is used for training or testing.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.classes = classes\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "qd_train_dataset = QuickDrawDataset(qd_train_raw_data, classes, True)\n",
    "qd_test_dataset = QuickDrawDataset(qd_test_raw_data, classes, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b143e94-c785-47af-a372-1706ea532123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: {'airplane': 151623, 'ant': 124612, 'axe': 124122, 'bed': 113862}\n",
      "\n",
      "0 {'ink': tensor([[-0.2292,  0.2328,  0.0000],\n",
      "        [-0.1146,  0.1422,  0.0000],\n",
      "        [-0.0435,  0.0216,  0.0000],\n",
      "        [-0.0435, -0.0129,  0.0000],\n",
      "        [-0.1067,  0.0000,  0.0000],\n",
      "        [ 0.1028, -0.0991,  0.0000],\n",
      "        [ 0.2372, -0.3534,  0.0000],\n",
      "        [-0.0711, -0.0043,  0.0000],\n",
      "        [-0.1858,  0.1121,  0.0000],\n",
      "        [-0.0870,  0.0345,  0.0000],\n",
      "        [-0.0949,  0.0086,  0.0000],\n",
      "        [-0.0198, -0.0259,  0.0000],\n",
      "        [ 0.0040, -0.1853,  0.0000],\n",
      "        [ 0.2530, -0.1207,  0.0000],\n",
      "        [ 0.1265, -0.0216,  0.0000],\n",
      "        [ 0.6126, -0.0129,  0.0000],\n",
      "        [ 0.0040,  0.1509,  0.0000],\n",
      "        [-0.0316,  0.0517,  0.0000],\n",
      "        [-0.2530,  0.0259,  0.0000],\n",
      "        [-0.0672, -0.0302,  1.0000],\n",
      "        [ 0.1660, -0.1983,  0.0000],\n",
      "        [ 0.0000,  0.1250,  0.0000],\n",
      "        [ 0.0119,  0.0474,  0.0000],\n",
      "        [ 0.0435,  0.0603,  0.0000],\n",
      "        [ 0.0672,  0.0302,  1.0000],\n",
      "        [-0.4704, -0.2716,  0.0000],\n",
      "        [-0.0474, -0.1336,  0.0000],\n",
      "        [-0.1225, -0.1638,  0.0000],\n",
      "        [-0.0237, -0.0129,  0.0000],\n",
      "        [-0.0830,  0.0043,  0.0000],\n",
      "        [-0.0395,  0.0172,  0.0000],\n",
      "        [ 0.0830,  0.1422,  0.0000],\n",
      "        [ 0.1304,  0.1379,  1.0000]]), 'className': 'airplane', 'shape': torch.Size([33, 3]), 'classIndex': 0}\n",
      "0 {'ink': tensor([[ 0.0039,  0.0890,  0.0000],\n",
      "        [-0.0157,  0.1507,  0.0000],\n",
      "        [-0.0078,  0.2945,  0.0000],\n",
      "        [ 0.0039,  0.2877,  0.0000],\n",
      "        [ 0.0314,  0.1027,  0.0000],\n",
      "        [ 0.0431,  0.0411,  0.0000],\n",
      "        [ 0.0314, -0.0205,  0.0000],\n",
      "        [ 0.0078, -0.0890,  0.0000],\n",
      "        [ 0.0000, -0.1575,  0.0000],\n",
      "        [-0.0392, -0.4178,  0.0000],\n",
      "        [-0.0667, -0.3151,  1.0000],\n",
      "        [ 0.0510,  0.3082,  0.0000],\n",
      "        [ 0.5686,  0.1096,  0.0000],\n",
      "        [-0.1333,  0.0822,  0.0000],\n",
      "        [-0.4510,  0.0822,  1.0000],\n",
      "        [-0.0431, -0.2397,  0.0000],\n",
      "        [-0.0510, -0.0137,  0.0000],\n",
      "        [-0.2549,  0.0479,  0.0000],\n",
      "        [-0.0510,  0.0274,  0.0000],\n",
      "        [-0.0157,  0.0342,  0.0000],\n",
      "        [ 0.2431,  0.1301,  0.0000],\n",
      "        [ 0.1098,  0.0137,  1.0000]]), 'className': 'airplane', 'shape': torch.Size([22, 3]), 'classIndex': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Classes:\", qd_train_dataset.classes)\n",
    "print()\n",
    "\n",
    "for i, sample in enumerate(qd_train_dataset):\n",
    "    print(i, sample)\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "for i, sample in enumerate(qd_test_dataset):\n",
    "    print(i, sample)\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebfb11b5-3c22-4a7b-b5fc-69485e7b5457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[-0.2205,  0.0000,  0.0000],\n",
      "        [-0.0591,  0.0136,  0.0000],\n",
      "        [-0.0354,  0.0682,  0.0000],\n",
      "        [-0.0039,  0.0636,  0.0000],\n",
      "        [ 0.0787,  0.0409,  0.0000],\n",
      "        [ 0.0984,  0.0682,  0.0000],\n",
      "        [ 0.1575,  0.0500,  0.0000],\n",
      "        [ 0.0354,  0.0227,  0.0000],\n",
      "        [ 0.2283,  0.0136,  0.0000],\n",
      "        [ 0.0709, -0.0500,  0.0000],\n",
      "        [ 0.0236, -0.1136,  0.0000],\n",
      "        [-0.0039, -0.0500,  0.0000],\n",
      "        [-0.0906, -0.1727,  0.0000],\n",
      "        [-0.0906, -0.0545,  0.0000],\n",
      "        [-0.0669, -0.0045,  0.0000],\n",
      "        [-0.0276,  0.0045,  0.0000],\n",
      "        [-0.0276,  0.0273,  0.0000],\n",
      "        [-0.0394,  0.0636,  1.0000],\n",
      "        [ 0.5079,  0.0500,  0.0000],\n",
      "        [-0.0551, -0.0091,  0.0000],\n",
      "        [-0.0630,  0.0182,  0.0000],\n",
      "        [-0.0866,  0.1091,  0.0000],\n",
      "        [-0.0039,  0.1091,  0.0000],\n",
      "        [ 0.0630,  0.0545,  0.0000],\n",
      "        [ 0.0630,  0.0182,  0.0000],\n",
      "        [ 0.1299,  0.0000,  0.0000],\n",
      "        [ 0.0276, -0.0136,  0.0000],\n",
      "        [ 0.0591, -0.0682,  0.0000],\n",
      "        [ 0.0118, -0.0364,  0.0000],\n",
      "        [ 0.0000, -0.1136,  0.0000],\n",
      "        [-0.0157, -0.0682,  0.0000],\n",
      "        [-0.0394, -0.0273,  1.0000],\n",
      "        [-0.1732,  0.0182,  0.0000],\n",
      "        [-0.0276,  0.0000,  0.0000],\n",
      "        [-0.0079, -0.0500,  0.0000],\n",
      "        [-0.0354, -0.0864,  0.0000],\n",
      "        [-0.0157, -0.1091,  1.0000],\n",
      "        [ 0.2559,  0.1818,  0.0000],\n",
      "        [ 0.0118, -0.1455,  0.0000],\n",
      "        [ 0.0197, -0.0455,  0.0000],\n",
      "        [ 0.0000, -0.0545,  0.0000],\n",
      "        [ 0.0276, -0.1182,  1.0000],\n",
      "        [-0.8110,  0.6227,  0.0000],\n",
      "        [ 0.0197,  0.0318,  0.0000],\n",
      "        [ 0.0039,  0.0364,  1.0000],\n",
      "        [ 0.1220, -0.0545,  0.0000],\n",
      "        [-0.0276,  0.2591,  1.0000],\n",
      "        [ 0.1850, -0.1909,  0.0000],\n",
      "        [ 0.0394,  0.0818,  0.0000],\n",
      "        [ 0.0000,  0.0909,  0.0000],\n",
      "        [ 0.0079,  0.0273,  0.0000],\n",
      "        [ 0.0118, -0.0045,  1.0000],\n",
      "        [ 0.0709, -0.2364,  0.0000],\n",
      "        [ 0.0276,  0.0045,  0.0000],\n",
      "        [ 0.0236,  0.0409,  0.0000],\n",
      "        [ 0.0039,  0.1818,  0.0000],\n",
      "        [ 0.0236,  0.0500,  0.0000],\n",
      "        [ 0.0039,  0.0591,  1.0000],\n",
      "        [-0.5551, -0.4955,  0.0000],\n",
      "        [-0.0433,  0.0455,  0.0000],\n",
      "        [-0.0551,  0.1045,  0.0000],\n",
      "        [-0.0394,  0.1273,  1.0000]], device='cuda:0') tensor([[62,  3],\n",
      "        [38,  3],\n",
      "        [20,  3],\n",
      "        [19,  3],\n",
      "        [27,  3],\n",
      "        [24,  3],\n",
      "        [49,  3],\n",
      "        [52,  3]], device='cuda:0') tensor([62, 38, 20, 19, 27, 24, 49, 52], device='cuda:0') tensor([1, 1, 3, 2, 0, 2, 1, 3], device='cuda:0')\n",
      "\n",
      "1 tensor([[ 0.0041,  0.8071,  0.0000],\n",
      "        [ 0.0697,  0.0000,  0.0000],\n",
      "        [ 0.0041, -0.2717,  0.0000],\n",
      "        [-0.0123, -0.0394,  0.0000],\n",
      "        [ 0.0164, -0.0591,  0.0000],\n",
      "        [ 0.0000, -0.3780,  0.0000],\n",
      "        [-0.0082, -0.0276,  0.0000],\n",
      "        [-0.0615,  0.0039,  0.0000],\n",
      "        [-0.0123, -0.0197,  0.0000],\n",
      "        [ 0.2377, -0.0866,  0.0000],\n",
      "        [ 0.1598,  0.0118,  0.0000],\n",
      "        [ 0.0820,  0.0276,  0.0000],\n",
      "        [ 0.0574,  0.0433,  0.0000],\n",
      "        [ 0.0287,  0.1142,  0.0000],\n",
      "        [ 0.0041,  0.0748,  0.0000],\n",
      "        [-0.0410,  0.0551,  0.0000],\n",
      "        [-0.0615,  0.0079,  0.0000],\n",
      "        [-0.0082,  0.0157,  0.0000],\n",
      "        [ 0.0369,  0.0197,  0.0000],\n",
      "        [ 0.0000,  0.0354,  0.0000],\n",
      "        [-0.0123,  0.0157,  0.0000],\n",
      "        [-0.1393,  0.0472,  0.0000],\n",
      "        [-0.1352,  0.0039,  0.0000],\n",
      "        [-0.1393, -0.0748,  0.0000],\n",
      "        [-0.0656,  0.0197,  0.0000],\n",
      "        [-0.3197,  0.0000,  0.0000],\n",
      "        [-0.0574, -0.0512,  0.0000],\n",
      "        [-0.0533, -0.1102,  0.0000],\n",
      "        [-0.0041, -0.2126,  0.0000],\n",
      "        [ 0.1107, -0.1535,  0.0000],\n",
      "        [ 0.1107, -0.0118,  0.0000],\n",
      "        [ 0.1189,  0.1142,  0.0000],\n",
      "        [ 0.0574,  0.0945,  0.0000],\n",
      "        [ 0.2213,  0.0236,  1.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000]], device='cuda:0') tensor([[34,  3],\n",
      "        [50,  3],\n",
      "        [25,  3],\n",
      "        [26,  3],\n",
      "        [22,  3],\n",
      "        [24,  3],\n",
      "        [21,  3],\n",
      "        [21,  3]], device='cuda:0') tensor([34, 50, 25, 26, 22, 24, 21, 21], device='cuda:0') tensor([2, 1, 2, 0, 3, 3, 2, 2], device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def quickDrawCollateFn(batch, batch_size):\n",
    "    shapes = [sample[\"shape\"] for sample in batch]\n",
    "    maxLen = max([shape[0] for shape in shapes])\n",
    "\n",
    "    ## Makes a dictionary of lists\n",
    "    newBatch = {\n",
    "        \"ink\": torch.zeros((batch_size, maxLen, 3)),\n",
    "        \"shape\": torch.zeros((batch_size, 2), dtype=int),\n",
    "        \"length\": torch.zeros((batch_size), dtype=int),\n",
    "        \"className\": [],\n",
    "        \"classIndex\": torch.zeros((batch_size), dtype=int),\n",
    "        \"maxLen\": maxLen\n",
    "    }\n",
    "    for i, sample in enumerate(batch):\n",
    "        newBatch[\"className\"].append(sample[\"className\"])\n",
    "        newBatch[\"classIndex\"][i] = sample[\"classIndex\"]\n",
    "        newBatch[\"shape\"][i] = torch.FloatTensor(list(sample[\"shape\"]))\n",
    "        newBatch[\"length\"][i] = sample[\"shape\"][0]\n",
    "\n",
    "        # Makes a copy of the tensor\n",
    "        newInk = F.pad(sample[\"ink\"], (0, 0, 0, maxLen - sample[\"shape\"][0]))\n",
    "        newBatch[\"ink\"][i] = newInk\n",
    "    \n",
    "    return newBatch\n",
    "\n",
    "qd_train_dataloader = DataLoader(qd_train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                                 num_workers=0, generator=torch.Generator(device='cuda'),\n",
    "                                 collate_fn=functools.partial(quickDrawCollateFn, batch_size=batch_size))\n",
    "\n",
    "qd_test_dataloader = DataLoader(qd_test_dataset, batch_size=batch_size, shuffle=False, \n",
    "                                num_workers=0, generator=torch.Generator(device='cuda'),\n",
    "                                collate_fn=functools.partial(quickDrawCollateFn, batch_size=batch_size))\n",
    "\n",
    "qd_eval_dataloader = DataLoader(qd_test_dataset, batch_size=1, shuffle=True, \n",
    "                                num_workers=0, generator=torch.Generator(device='cuda'),\n",
    "                                collate_fn=functools.partial(quickDrawCollateFn, batch_size=1))\n",
    "\n",
    "for i_batch, sample_batched in enumerate(qd_train_dataloader):\n",
    "    print(i_batch, sample_batched[\"ink\"][0], sample_batched[\"shape\"], sample_batched[\"length\"], sample_batched[\"classIndex\"])\n",
    "    print()\n",
    "\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0425ca-c5e9-4176-988d-5bbaa65e8a71",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Defining The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29828fe2-8d67-4f01-a8ff-f0e816864d16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "# Thanks to: https://discuss.pytorch.org/t/pytorch-equivalent-for-tf-sequence-mask/39036/2\n",
    "def sequence_mask(lengths, maxlen = None, dtype=torch.bool):\n",
    "    if maxlen is None:\n",
    "        maxlen = lengths.max()\n",
    "    mask = ~(torch.ones((len(lengths), maxlen)).cumsum(dim=1).t() > lengths).t()\n",
    "    mask.type(dtype)\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "243f307a-d433-48cd-a57a-75ed25acb43f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: {'airplane': 151623, 'ant': 124612, 'axe': 124122, 'bed': 113862}\n",
      "Class count: 4\n"
     ]
    }
   ],
   "source": [
    "class QuickDrawRNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, classes):\n",
    "        super(QuickDrawRNN, self).__init__()\n",
    "\n",
    "        # Init data\n",
    "        classCnt = len(classes)\n",
    "\n",
    "        # 3x 1D Convolutions\n",
    "        \n",
    "        # Filters: [48, 64, 96]\n",
    "        # Length of convolutional filters: [5, 5, 3]\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d( 3, 48, 5, stride=1, padding=2),  # Should we disable bias?\n",
    "            torch.nn.Dropout(p=dropout_rate),\n",
    "            torch.nn.Conv1d(48, 64, 5, stride=1, padding=2),\n",
    "            torch.nn.Dropout(p=dropout_rate),\n",
    "            torch.nn.Conv1d(64, 96, 3, stride=1, padding=1)\n",
    "        )\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "\n",
    "        # Num layers: num_layers (3)\n",
    "        # Num nodes: num_nodes (128)\n",
    "        # Dropout = dropout_rate if TRAIN else 0\n",
    "        # Direction = bidirectional\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            96, \n",
    "            num_nodes, \n",
    "            num_layers=num_layers, \n",
    "            bias=True,    # Should this be false?\n",
    "            batch_first=True, \n",
    "            dropout=dropout_rate, \n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Fully Connected\n",
    "\n",
    "        # Input: 2 * num_nodes (256)\n",
    "        # Output: Number of classes\n",
    "        print(\"Classes:\", classes)\n",
    "        print(\"Class count:\", classCnt)\n",
    "        \n",
    "        self.fc = torch.nn.Linear(num_nodes * 2, classCnt)\n",
    "        \n",
    "\n",
    "    def forward(self, inks, lengths):\n",
    "        \n",
    "        # print(inks.shape)\n",
    "        # print(lengths)\n",
    "        # print()\n",
    "\n",
    "        # conv\n",
    "        inks = self.conv(inks.permute(0, 2, 1))\n",
    "        \n",
    "        # permute inks back\n",
    "        inks = inks.permute(0, 2, 1)\n",
    "\n",
    "        # Inks should now be of shape: (B, L, convFilters[3] (default 96))\n",
    "        # print(inks.shape)\n",
    "        \n",
    "        # lstm\n",
    "        inks, _ = self.lstm(inks)\n",
    "\n",
    "        # Inks should now be of shape: (B, L, 2 * num_nodes (default 2 * 128, the 'times 2' is because bidir LSTM doubles the features/nodes))\n",
    "        # print(inks.shape)\n",
    "\n",
    "        # mask to remove the data past the initial length of each drawing\n",
    "        mask = torch.tile(\n",
    "            torch.unsqueeze(sequence_mask(lengths, inks.shape[1]), 2),\n",
    "            (1, 1, inks.shape[2])\n",
    "        )\n",
    "\n",
    "        # print()\n",
    "        # print(\"Mask:\", mask.shape)\n",
    "\n",
    "        inks_maked = torch.where(mask, inks, torch.zeros_like(inks))\n",
    "        # print(inks_maked.shape)\n",
    "\n",
    "        inks = torch.sum(inks_maked, dim=1)\n",
    "\n",
    "        # Inks should now be of shape: (B, 2 * num_nodes)\n",
    "        # print(inks.shape)\n",
    "        # print()\n",
    "\n",
    "        # fc\n",
    "        inks = self.fc(inks)\n",
    "\n",
    "        # Inks should now be of shape: (B, num_classes)\n",
    "        # print(inks.shape)\n",
    "        # print()\n",
    "\n",
    "        return inks\n",
    "        \n",
    "        \n",
    "        # embeds = self.word_embeddings(sentence)\n",
    "        # lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        # tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        # tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        # return tag_scores\n",
    "\n",
    "qd_model = QuickDrawRNN(qd_train_dataset.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37de5646-69cc-4ec8-a68b-6ea866151030",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, qd_loader, optimizer, loss_fn, epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    \n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    print(len(qd_loader))\n",
    "    for i, batch in enumerate(qd_loader):\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make predictions for this batch\n",
    "        logits = model(batch[\"ink\"], batch[\"length\"])\n",
    "        \n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(logits, batch[\"classIndex\"])\n",
    "        loss.backward()\n",
    "\n",
    "        # print(\"Loss:\", loss.item())\n",
    "        \n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {}/{} loss: {}'.format(i + 1, len(qd_loader), last_loss))\n",
    "            tb_x = epoch_index * len(qd_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "        \n",
    "        # print()\n",
    "        # print()\n",
    "        # print()\n",
    "        # print()\n",
    "            \n",
    "    return last_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e412252e-df8c-4e02-902d-3423cd003a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(qd_model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86c3394c-7e69-40de-9516-bf260aa0714f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "58528\n",
      "  batch 1000/58528 loss: 1.2873169038891792\n",
      "  batch 2000/58528 loss: 1.051373823940754\n",
      "  batch 3000/58528 loss: 0.9791011981368065\n",
      "  batch 4000/58528 loss: 0.8934804668426514\n",
      "  batch 5000/58528 loss: 0.8226561665982008\n",
      "  batch 6000/58528 loss: 0.7095819232165813\n",
      "  batch 7000/58528 loss: 0.6718349764049053\n",
      "  batch 8000/58528 loss: 0.6006119401976466\n",
      "  batch 9000/58528 loss: 0.5546294914782047\n",
      "  batch 10000/58528 loss: 0.4940850682742894\n",
      "  batch 11000/58528 loss: 0.48000131288170816\n",
      "  batch 12000/58528 loss: 0.4424793520085514\n",
      "  batch 13000/58528 loss: 0.42730687057971956\n",
      "  batch 14000/58528 loss: 0.39149605928966774\n",
      "  batch 15000/58528 loss: 0.37618063345970587\n",
      "  batch 16000/58528 loss: 0.35091781592415644\n",
      "  batch 17000/58528 loss: 0.3617684913915582\n",
      "  batch 18000/58528 loss: 0.3085946528383065\n",
      "  batch 19000/58528 loss: 0.34734545040223747\n",
      "  batch 20000/58528 loss: 0.32087643685610967\n",
      "  batch 21000/58528 loss: 0.30544851732684764\n",
      "  batch 22000/58528 loss: 0.2967808214400429\n",
      "  batch 23000/58528 loss: 0.2954874809582252\n",
      "  batch 24000/58528 loss: 0.2727462514065555\n",
      "  batch 25000/58528 loss: 0.28597281125746665\n",
      "  batch 26000/58528 loss: 0.2652200446142815\n",
      "  batch 27000/58528 loss: 0.2562616724293912\n",
      "  batch 28000/58528 loss: 0.2583195141536416\n",
      "  batch 29000/58528 loss: 0.256868651275523\n",
      "  batch 30000/58528 loss: 0.24125506348744966\n",
      "  batch 31000/58528 loss: 0.25225448846723886\n",
      "  batch 32000/58528 loss: 0.23298954518325626\n",
      "  batch 33000/58528 loss: 0.24201536263985327\n",
      "  batch 34000/58528 loss: 0.22548008799320088\n",
      "  batch 35000/58528 loss: 0.2310556201110012\n",
      "  batch 36000/58528 loss: 0.21296276457526256\n",
      "  batch 37000/58528 loss: 0.21756587811384814\n",
      "  batch 38000/58528 loss: 0.20610218031634578\n",
      "  batch 39000/58528 loss: 0.20921288300771265\n",
      "  batch 40000/58528 loss: 0.20199708479260153\n",
      "  batch 41000/58528 loss: 0.19313585149680149\n",
      "  batch 42000/58528 loss: 0.20878927834774366\n",
      "  batch 43000/58528 loss: 0.18871256807941245\n",
      "  batch 44000/58528 loss: 0.19731208523357055\n",
      "  batch 45000/58528 loss: 0.19611031998170075\n",
      "  batch 46000/58528 loss: 0.18015617356525035\n",
      "  batch 47000/58528 loss: 0.18135748358105774\n",
      "  batch 48000/58528 loss: 0.17977733384730527\n",
      "  batch 49000/58528 loss: 0.16696368013857865\n",
      "  batch 50000/58528 loss: 0.17024724160553886\n",
      "  batch 51000/58528 loss: 0.1675222423556552\n",
      "  batch 52000/58528 loss: 0.16667877954803406\n",
      "  batch 53000/58528 loss: 0.16129674005904235\n",
      "  batch 54000/58528 loss: 0.17684995413722937\n",
      "  batch 55000/58528 loss: 0.1636267390837893\n",
      "  batch 56000/58528 loss: 0.1589141922035633\n",
      "  batch 57000/58528 loss: 0.16412245433125644\n",
      "  batch 58000/58528 loss: 0.15761665416602044\n",
      "\n",
      "Beginning validation!\n",
      "  batch 1000/5750 vloss: 0.1181020587682724\n",
      "  batch 2000/5750 vloss: 0.11187874525785446\n",
      "  batch 3000/5750 vloss: 0.09927070885896683\n",
      "  batch 4000/5750 vloss: 0.11932221800088882\n",
      "  batch 5000/5750 vloss: 0.13138499855995178\n",
      "LOSS train 0.15761665416602044 valid 0.13707198202610016\n",
      "\n",
      "\n",
      "EPOCH 2:\n",
      "58528\n",
      "  batch 1000/58528 loss: 0.15109308993068407\n",
      "  batch 2000/58528 loss: 0.13315611364276264\n",
      "  batch 3000/58528 loss: 0.15085864978213795\n",
      "  batch 4000/58528 loss: 0.15536513346066932\n",
      "  batch 5000/58528 loss: 0.14253484964134985\n",
      "  batch 6000/58528 loss: 0.1571536038025224\n",
      "  batch 7000/58528 loss: 0.14458797767142822\n",
      "  batch 8000/58528 loss: 0.14017616177829767\n",
      "  batch 9000/58528 loss: 0.13693658977069573\n",
      "  batch 10000/58528 loss: 0.1462480973778147\n",
      "  batch 11000/58528 loss: 0.13607899853843264\n",
      "  batch 12000/58528 loss: 0.13362279432086507\n",
      "  batch 13000/58528 loss: 0.1441608771121537\n",
      "  batch 14000/58528 loss: 0.13550790660447093\n",
      "  batch 15000/58528 loss: 0.12633186903934984\n",
      "  batch 16000/58528 loss: 0.14170832184945176\n",
      "  batch 17000/58528 loss: 0.12868442917475476\n",
      "  batch 18000/58528 loss: 0.1261882437375316\n",
      "  batch 19000/58528 loss: 0.11908253257987962\n",
      "  batch 20000/58528 loss: 0.12835683359513494\n",
      "  batch 21000/58528 loss: 0.1231288215094537\n",
      "  batch 22000/58528 loss: 0.13691276699560695\n",
      "  batch 23000/58528 loss: 0.13275333073956427\n",
      "  batch 24000/58528 loss: 0.12071776594648691\n",
      "  batch 25000/58528 loss: 0.1311311711233284\n",
      "  batch 26000/58528 loss: 0.13136066987169034\n",
      "  batch 27000/58528 loss: 0.11781104114231129\n",
      "  batch 28000/58528 loss: 0.12888998992115375\n",
      "  batch 29000/58528 loss: 0.11648538182173797\n",
      "  batch 30000/58528 loss: 0.11548493003677868\n",
      "  batch 31000/58528 loss: 0.1320608174633235\n",
      "  batch 32000/58528 loss: 0.12781172023627732\n",
      "  batch 33000/58528 loss: 0.12823265670915135\n",
      "  batch 34000/58528 loss: 0.11665764384418435\n",
      "  batch 35000/58528 loss: 0.11528803578093356\n",
      "  batch 36000/58528 loss: 0.11474959165416658\n",
      "  batch 37000/58528 loss: 0.11547481763847463\n",
      "  batch 38000/58528 loss: 0.1195211119981177\n",
      "  batch 39000/58528 loss: 0.11850763409209503\n",
      "  batch 40000/58528 loss: 0.11233722052493976\n",
      "  batch 41000/58528 loss: 0.10906075619820331\n",
      "  batch 42000/58528 loss: 0.11432482458384766\n",
      "  batch 43000/58528 loss: 0.11994165707471256\n",
      "  batch 44000/58528 loss: 0.11285011233294791\n",
      "  batch 45000/58528 loss: 0.11406771041214234\n",
      "  batch 46000/58528 loss: 0.10695825187892478\n",
      "  batch 47000/58528 loss: 0.10441178384151863\n",
      "  batch 48000/58528 loss: 0.10245409773410938\n",
      "  batch 49000/58528 loss: 0.11049800121976296\n",
      "  batch 50000/58528 loss: 0.0994006509940209\n",
      "  batch 51000/58528 loss: 0.11909250251034974\n",
      "  batch 52000/58528 loss: 0.11257135098871368\n",
      "  batch 53000/58528 loss: 0.10603380239583203\n",
      "  batch 54000/58528 loss: 0.10224292235411121\n",
      "  batch 55000/58528 loss: 0.11465437225177448\n",
      "  batch 56000/58528 loss: 0.10807811289882374\n",
      "  batch 57000/58528 loss: 0.11320502816692533\n",
      "  batch 58000/58528 loss: 0.11038335710391402\n",
      "\n",
      "Beginning validation!\n",
      "  batch 1000/5750 vloss: 0.057487837970256805\n",
      "  batch 2000/5750 vloss: 0.059801552444696426\n",
      "  batch 3000/5750 vloss: 0.05650218203663826\n",
      "  batch 4000/5750 vloss: 0.07778307795524597\n",
      "  batch 5000/5750 vloss: 0.0874001681804657\n",
      "LOSS train 0.11038335710391402 valid 0.09122896194458008\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/quick_draw_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    \n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    qd_model.train(True)\n",
    "    avg_loss = train_one_epoch(qd_model, qd_train_dataloader, optimizer, loss_fn, epoch_number, writer)\n",
    "    \n",
    "    # We don't need gradients on to do reporting\n",
    "    qd_model.train(False)\n",
    "\n",
    "    print()\n",
    "    print(\"Beginning validation!\")\n",
    "    \n",
    "    running_vloss = 0.0\n",
    "    for i, vbatch in enumerate(qd_test_dataloader):\n",
    "        voutputs = qd_model(vbatch[\"ink\"], vbatch[\"length\"])\n",
    "        vloss = loss_fn(voutputs, vbatch[\"classIndex\"])\n",
    "        running_vloss += vloss\n",
    "\n",
    "        \n",
    "        if i % 1000 == 999:\n",
    "            last_vloss = running_vloss / (i + 1)\n",
    "            print('  batch {}/{} vloss: {}'.format(i + 1, len(qd_test_dataloader), last_vloss))\n",
    "    \n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    \n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch_number + 1)\n",
    "    writer.flush()\n",
    "    \n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'models/model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(qd_model.state_dict(), model_path)\n",
    "    \n",
    "    epoch_number += 1\n",
    "\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710d4b01-3cbb-4abb-b915-eae2d69ff9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
