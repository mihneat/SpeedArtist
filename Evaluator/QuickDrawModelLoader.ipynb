{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6b53fbb-5cb0-4d1a-9a20-58612345850a",
   "metadata": {},
   "source": [
    "# Quick Draw Model Loader - PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7888a0-3abe-446b-bfe2-bffc2bfb34cd",
   "metadata": {},
   "source": [
    "## <font color='yellow'>Attention! This is the LOADER version of the model, use `QuickDrawModel.ipynb` if you want to train the model!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3286cfd-c6de-491f-8cb4-0124fed42654",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e42190-823f-44b6-b365-e527fcc9a6cd",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44ff9e5b-8c7b-42af-b762-7b2052ebfd96",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full power!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import json\n",
    "from datetime import datetime\n",
    "import functools\n",
    "import sklearn as sk\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Full power!\")\n",
    "    dev = torch.set_default_device(\"cuda\")\n",
    "else:\n",
    "    print(\"Regular power..\")\n",
    "    dev = torch.set_default_device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68710809-299a-4492-a0c0-43da5e7b684b",
   "metadata": {},
   "source": [
    "### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef25a92c-9181-4b3c-843d-88520c07ece3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.get_default_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ed76bd-4cd0-4044-b026-5009771eb375",
   "metadata": {},
   "source": [
    "## Manipulating The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373a13d0-632a-4481-a92d-9f6a34691283",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a7e525a-8859-4954-a502-2861db9d4971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some useful global variables\n",
    "\n",
    "classes = {}\n",
    "batch_size = 8\n",
    "eval_batch_size = 8\n",
    "dropout_rate = 0.3\n",
    "num_layers = 3\n",
    "num_nodes = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "276fd709-d169-499c-8913-fe9d958d8c0f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "classes = {}\n",
    "\n",
    "def parseLine(ndjsonLine):\n",
    "  \"\"\"Parse an ndjson line and return ink (as np array) and classname.\"\"\"\n",
    "  sample = json.loads(ndjsonLine)\n",
    "  class_name = sample[\"word\"]\n",
    "  inkarray = sample[\"drawing\"]\n",
    "  stroke_lengths = [len(stroke[0]) for stroke in inkarray]\n",
    "  total_points = sum(stroke_lengths)\n",
    "  np_ink = np.zeros((total_points, 3), dtype=np.float32)\n",
    "  current_t = 0\n",
    "  for stroke in inkarray:\n",
    "    for i in [0, 1]:\n",
    "      np_ink[current_t:(current_t + len(stroke[0])), i] = stroke[i]\n",
    "    current_t += len(stroke[0])\n",
    "    np_ink[current_t - 1, 2] = 1  # stroke_end\n",
    "      \n",
    "  # Preprocessing.\n",
    "  # 1. Size normalization.\n",
    "  lower = np.min(np_ink[:, 0:2], axis=0)\n",
    "  upper = np.max(np_ink[:, 0:2], axis=0)\n",
    "  scale = upper - lower\n",
    "  scale[scale == 0] = 1\n",
    "  np_ink[:, 0:2] = (np_ink[:, 0:2] - lower) / scale\n",
    "    \n",
    "  # 2. Compute deltas.\n",
    "  np_ink[1:, 0:2] -= np_ink[0:-1, 0:2]\n",
    "  np_ink = np_ink[1:, :]\n",
    "  return torch.from_numpy(np_ink), class_name\n",
    "\n",
    "def readData(files, train_data, test_data, limit = -1):\n",
    "    # Clear the global variables\n",
    "    classes = {}\n",
    "    \n",
    "    filesToParse = files if limit < 0 else files[:limit]\n",
    "\n",
    "    currClassIndex = 0\n",
    "    classNameToIndex = {}\n",
    "    \n",
    "    cnt = 0\n",
    "    sampleCnt = 0\n",
    "    for filePath in filesToParse:        \n",
    "        with open(filePath) as file:\n",
    "            for line in file:\n",
    "                # sample = json.loads(line)\n",
    "                # className = sample[\"word\"]\n",
    "                features = {}\n",
    "                features[\"ink\"], features[\"className\"] = parseLine(line)\n",
    "\n",
    "                # Define the shape of the ink\n",
    "                features[\"shape\"] = features[\"ink\"].shape\n",
    "\n",
    "                # Index the class\n",
    "                if features[\"className\"] not in classNameToIndex:\n",
    "                    classNameToIndex[features[\"className\"]] = currClassIndex\n",
    "                    currClassIndex += 1\n",
    "\n",
    "                features[\"classIndex\"] = classNameToIndex[features[\"className\"]]\n",
    "\n",
    "                # Keep a class statistic\n",
    "                if features[\"className\"] not in classes:\n",
    "                    classes[features[\"className\"]] = 0\n",
    "\n",
    "                classes[features[\"className\"]] += 1\n",
    "\n",
    "                if sampleCnt % 11000 < 10000:\n",
    "                    train_data.append(features)\n",
    "                else:\n",
    "                    test_data.append(features)\n",
    "\n",
    "                sampleCnt += 1\n",
    "\n",
    "        cnt += 1\n",
    "\n",
    "        print(\"Finished parsing {0}/{1}: {2}\".format(cnt, len(files), filePath))\n",
    "\n",
    "    print(\"Finished parsing all the data!\")\n",
    "    return classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db4bd842-cfdd-45f8-a4ee-d2de5ba0eb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished parsing 1/16: datasets/full_simplified_airplane.ndjson\n",
      "Finished parsing 2/16: datasets/full_simplified_ant.ndjson\n",
      "Finished parsing 3/16: datasets/full_simplified_axe.ndjson\n",
      "Finished parsing 4/16: datasets/full_simplified_bed.ndjson\n",
      "Finished parsing all the data!\n",
      "\n",
      "{'airplane': 151623, 'ant': 124612, 'axe': 124122, 'bed': 113862}\n",
      "Train data len: 468219\n",
      "Test data len: 46000\n"
     ]
    }
   ],
   "source": [
    "qd_train_raw_data = []\n",
    "qd_test_raw_data = []\n",
    "\n",
    "root_dir = \"datasets\"\n",
    "\n",
    "dataFiles = [root_dir + \"/\" + f for f in listdir(root_dir) if f.endswith(\".ndjson\")]\n",
    "classes = readData(dataFiles, qd_train_raw_data, qd_test_raw_data, limit=4)\n",
    "\n",
    "print()\n",
    "print(classes)\n",
    "print(\"Train data len:\", len(qd_train_raw_data))\n",
    "print(\"Test data len:\", len(qd_test_raw_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4810f78b-e1e0-460d-b243-d0aeaa545b07",
   "metadata": {},
   "source": [
    "### Creating the Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7140d6dd-b651-4fe5-a650-090d3351af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickDrawDataset(Dataset):\n",
    "    \"\"\"Quick, Draw! data subset.\"\"\"\n",
    "\n",
    "    def __init__(self, data, classes, train):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            data (list): List of all the parsed data with the readData() function.\n",
    "            classes (dict): Dictionary with all the classes and how many of each there are.\n",
    "            train (bool): Says if the dataset is used for training or testing.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.classes = classes\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "qd_train_dataset = QuickDrawDataset(qd_train_raw_data, classes, True)\n",
    "qd_test_dataset = QuickDrawDataset(qd_test_raw_data, classes, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebfb11b5-3c22-4a7b-b5fc-69485e7b5457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quickDrawCollateFn(batch, batch_size):\n",
    "    shapes = [sample[\"shape\"] for sample in batch]\n",
    "    maxLen = max([shape[0] for shape in shapes])\n",
    "\n",
    "    ## Makes a dictionary of lists\n",
    "    newBatch = {\n",
    "        \"ink\": torch.zeros((batch_size, maxLen, 3)),\n",
    "        \"shape\": torch.zeros((batch_size, 2), dtype=int),\n",
    "        \"length\": torch.zeros((batch_size), dtype=int),\n",
    "        \"className\": [],\n",
    "        \"classIndex\": torch.zeros((batch_size), dtype=int),\n",
    "        \"maxLen\": maxLen\n",
    "    }\n",
    "    for i, sample in enumerate(batch):\n",
    "        newBatch[\"className\"].append(sample[\"className\"])\n",
    "        newBatch[\"classIndex\"][i] = sample[\"classIndex\"]\n",
    "        newBatch[\"shape\"][i] = torch.FloatTensor(list(sample[\"shape\"]))\n",
    "        newBatch[\"length\"][i] = sample[\"shape\"][0]\n",
    "\n",
    "        # Makes a copy of the tensor\n",
    "        newInk = F.pad(sample[\"ink\"], (0, 0, 0, maxLen - sample[\"shape\"][0]))\n",
    "        newBatch[\"ink\"][i] = newInk\n",
    "    \n",
    "    return newBatch\n",
    "\n",
    "qd_train_dataloader = DataLoader(qd_train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                                 num_workers=0, generator=torch.Generator(device='cuda'),\n",
    "                                 collate_fn=functools.partial(quickDrawCollateFn, batch_size=batch_size))\n",
    "\n",
    "qd_test_dataloader = DataLoader(qd_test_dataset, batch_size=batch_size, shuffle=False, \n",
    "                                num_workers=0, generator=torch.Generator(device='cuda'),\n",
    "                                collate_fn=functools.partial(quickDrawCollateFn, batch_size=batch_size))\n",
    "\n",
    "qd_eval_dataloader = DataLoader(qd_test_dataset, batch_size=eval_batch_size, shuffle=True, \n",
    "                                num_workers=0, generator=torch.Generator(device='cuda'),\n",
    "                                collate_fn=functools.partial(quickDrawCollateFn, batch_size=eval_batch_size))\n",
    "\n",
    "# for i_batch, sample_batched in enumerate(qd_train_dataloader):\n",
    "#     print(i_batch, sample_batched[\"ink\"][0], sample_batched[\"shape\"], sample_batched[\"length\"], sample_batched[\"classIndex\"])\n",
    "#     print()\n",
    "\n",
    "#     # observe 4th batch and stop.\n",
    "#     if i_batch == 1:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0425ca-c5e9-4176-988d-5bbaa65e8a71",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Defining The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29828fe2-8d67-4f01-a8ff-f0e816864d16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "# Thanks to: https://discuss.pytorch.org/t/pytorch-equivalent-for-tf-sequence-mask/39036/2\n",
    "def sequence_mask(lengths, maxlen = None, dtype=torch.bool):\n",
    "    if maxlen is None:\n",
    "        maxlen = lengths.max()\n",
    "    mask = ~(torch.ones((len(lengths), maxlen)).cumsum(dim=1).t() > lengths).t()\n",
    "    mask.type(dtype)\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "243f307a-d433-48cd-a57a-75ed25acb43f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: {'airplane': 151623, 'ant': 124612, 'axe': 124122, 'bed': 113862}\n",
      "Class count: 4\n"
     ]
    }
   ],
   "source": [
    "class QuickDrawRNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, classes):\n",
    "        super(QuickDrawRNN, self).__init__()\n",
    "\n",
    "        # Init data\n",
    "        classCnt = len(classes)\n",
    "\n",
    "        # 3x 1D Convolutions\n",
    "        \n",
    "        # Filters: [48, 64, 96]\n",
    "        # Length of convolutional filters: [5, 5, 3]\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d( 3, 48, 5, stride=1, padding=2),  # Should we disable bias?\n",
    "            torch.nn.Dropout(p=dropout_rate),\n",
    "            torch.nn.Conv1d(48, 64, 5, stride=1, padding=2),\n",
    "            torch.nn.Dropout(p=dropout_rate),\n",
    "            torch.nn.Conv1d(64, 96, 3, stride=1, padding=1)\n",
    "        )\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "\n",
    "        # Num layers: num_layers (3)\n",
    "        # Num nodes: num_nodes (128)\n",
    "        # Dropout = dropout_rate if TRAIN else 0\n",
    "        # Direction = bidirectional\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            96, \n",
    "            num_nodes, \n",
    "            num_layers=num_layers, \n",
    "            bias=True,    # Should this be false?\n",
    "            batch_first=True, \n",
    "            dropout=dropout_rate, \n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Fully Connected\n",
    "\n",
    "        # Input: 2 * num_nodes (256)\n",
    "        # Output: Number of classes\n",
    "        print(\"Classes:\", classes)\n",
    "        print(\"Class count:\", classCnt)\n",
    "        \n",
    "        self.fc = torch.nn.Linear(num_nodes * 2, classCnt)\n",
    "        \n",
    "\n",
    "    def forward(self, inks, lengths):\n",
    "        \n",
    "        # print(inks.shape)\n",
    "        # print(lengths)\n",
    "        # print()\n",
    "\n",
    "        # conv\n",
    "        inks = self.conv(inks.permute(0, 2, 1))\n",
    "        \n",
    "        # permute inks back\n",
    "        inks = inks.permute(0, 2, 1)\n",
    "\n",
    "        # Inks should now be of shape: (B, L, convFilters[3] (default 96))\n",
    "        # print(inks.shape)\n",
    "        \n",
    "        # lstm\n",
    "        inks, _ = self.lstm(inks)\n",
    "\n",
    "        # Inks should now be of shape: (B, L, 2 * num_nodes (default 2 * 128, the 'times 2' is because bidir LSTM doubles the features/nodes))\n",
    "        # print(inks.shape)\n",
    "\n",
    "        # mask to remove the data past the initial length of each drawing\n",
    "        mask = torch.tile(\n",
    "            torch.unsqueeze(sequence_mask(lengths, inks.shape[1]), 2),\n",
    "            (1, 1, inks.shape[2])\n",
    "        )\n",
    "\n",
    "        # print()\n",
    "        # print(\"Mask:\", mask.shape)\n",
    "\n",
    "        inks_maked = torch.where(mask, inks, torch.zeros_like(inks))\n",
    "        # print(inks_maked.shape)\n",
    "\n",
    "        inks = torch.sum(inks_maked, dim=1)\n",
    "\n",
    "        # Inks should now be of shape: (B, 2 * num_nodes)\n",
    "        # print(inks.shape)\n",
    "        # print()\n",
    "\n",
    "        # fc\n",
    "        inks = self.fc(inks)\n",
    "\n",
    "        # Inks should now be of shape: (B, num_classes)\n",
    "        # print(inks.shape)\n",
    "        # print()\n",
    "\n",
    "        return inks\n",
    "        \n",
    "        \n",
    "        # embeds = self.word_embeddings(sentence)\n",
    "        # lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        # tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        # tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        # return tag_scores\n",
    "\n",
    "qd_model = QuickDrawRNN(qd_train_dataset.classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e09381-8f97-4dd4-8f5d-ec0b308223c3",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db126803-fd23-4f97-9af5-d7f7ccbe877f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuickDrawRNN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv1d(3, 48, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (1): Dropout(p=0.3, inplace=False)\n",
       "    (2): Conv1d(48, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Conv1d(64, 96, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  )\n",
       "  (lstm): LSTM(96, 128, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  (fc): Linear(in_features=256, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qd_model.load_state_dict(torch.load(\"models/model_20250411_222609_1\", weights_only=True))\n",
    "qd_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86c3394c-7e69-40de-9516-bf260aa0714f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evalInput(batch, y_true, y_pred):\n",
    "    # We don't need gradients on to do reporting\n",
    "    qd_model.train(False)\n",
    "\n",
    "    logits = qd_model(batch[\"ink\"], batch[\"length\"])\n",
    "\n",
    "    predicted_labels = torch.argmax(logits, dim=1)\n",
    "    actual_labels = batch[\"classIndex\"]\n",
    "\n",
    "    # Very inefficient, would love to just count true pos/neg, false pos/neg, but using scikit is fun :)\n",
    "    y_true += actual_labels.cpu()\n",
    "    y_pred += predicted_labels.cpu()\n",
    "    \n",
    "    # print(logits)\n",
    "    # print(\"Result:   \", predictedLabels)\n",
    "    # print(\"Expected: \", actualLabels)\n",
    "    # print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "710d4b01-3cbb-4abb-b915-eae2d69ff9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after 10/5750: 97.5000%\n",
      "Accuracy after 20/5750: 97.5000%\n",
      "Accuracy after 30/5750: 97.9167%\n",
      "Accuracy after 40/5750: 96.8750%\n",
      "Accuracy after 50/5750: 97.0000%\n",
      "Accuracy after 60/5750: 96.8750%\n",
      "Accuracy after 70/5750: 97.1429%\n",
      "Accuracy after 80/5750: 97.0312%\n",
      "Accuracy after 90/5750: 96.9444%\n",
      "Accuracy after 100/5750: 97.0000%\n",
      "Accuracy after 110/5750: 97.1591%\n",
      "Accuracy after 120/5750: 97.1875%\n",
      "Accuracy after 130/5750: 97.3077%\n",
      "Accuracy after 140/5750: 97.0536%\n",
      "Accuracy after 150/5750: 97.1667%\n",
      "\n",
      "Final accuracy: 0.9712171052631579\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = [], []\n",
    "for i, batch in enumerate(qd_eval_dataloader):\n",
    "    # print(batch)\n",
    "\n",
    "    evalInput(batch, y_true, y_pred)\n",
    "\n",
    "    # Intermediate accuracy\n",
    "    int_acc = sk.metrics.accuracy_score(y_true, y_pred)\n",
    "\n",
    "    if i % 10 == 9:\n",
    "        print(\"Accuracy after {}/{}: {:.4f}%\".format(i + 1, len(qd_eval_dataloader), int_acc * 100.))\n",
    "\n",
    "    if i > 150:\n",
    "        break\n",
    "\n",
    "acc = sk.metrics.accuracy_score(y_true, y_pred)\n",
    "\n",
    "print()\n",
    "print(\"Final accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba58d71-5ade-49f2-a93e-a91a033bb61c",
   "metadata": {},
   "source": [
    "### Per-class accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2b4d285-a96b-49c0-a2dc-22d17e0d56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalInput(batch, y_true, y_pred):\n",
    "    # We don't need gradients on to do reporting\n",
    "    qd_model.train(False)\n",
    "\n",
    "    logits = qd_model(batch[\"ink\"], batch[\"length\"])\n",
    "\n",
    "    predicted_labels = torch.argmax(logits, dim=1)\n",
    "    actual_labels = batch[\"classIndex\"]\n",
    "\n",
    "    # Very inefficient, would love to just count true pos/neg, false pos/neg, but using scikit is fun :)\n",
    "    true = actual_labels.cpu()\n",
    "    pred = predicted_labels.cpu()\n",
    "\n",
    "    for i, true_cls_raw in enumerate(true):\n",
    "        true_cls = true_cls_raw.item()\n",
    "\n",
    "        if true_cls not in class_cnt:\n",
    "            class_cnt[true_cls] = 0;\n",
    "            class_correct_cnt[true_cls] = 0\n",
    "        \n",
    "        class_cnt[true_cls] += 1\n",
    "        class_correct_cnt[true_cls] += 1 if pred[i].item() == true_cls else 0\n",
    "    \n",
    "    # print(logits)\n",
    "    # print(\"Result:   \", predictedLabels)\n",
    "    # print(\"Expected: \", actualLabels)\n",
    "    # print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "20627c99-0b52-4af4-bc4d-2528ff75f024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after 10/5750 for class 0: 100.0000%\n",
      "Accuracy after 10/5750 for class 1: 100.0000%\n",
      "Accuracy after 10/5750 for class 2: 96.2963%\n",
      "Accuracy after 10/5750 for class 3: 100.0000%\n",
      "\n",
      "Accuracy after 60/5750 for class 0: 97.0588%\n",
      "Accuracy after 60/5750 for class 1: 99.2537%\n",
      "Accuracy after 60/5750 for class 2: 95.6522%\n",
      "Accuracy after 60/5750 for class 3: 97.8947%\n",
      "\n",
      "Accuracy after 110/5750 for class 0: 97.4170%\n",
      "Accuracy after 110/5750 for class 1: 99.0868%\n",
      "Accuracy after 110/5750 for class 2: 94.8837%\n",
      "Accuracy after 110/5750 for class 3: 97.1429%\n",
      "\n",
      "Accuracy after 160/5750 for class 0: 97.8836%\n",
      "Accuracy after 160/5750 for class 1: 99.0881%\n",
      "Accuracy after 160/5750 for class 2: 95.1923%\n",
      "Accuracy after 160/5750 for class 3: 96.9349%\n",
      "\n",
      "Accuracy after 210/5750 for class 0: 97.9508%\n",
      "Accuracy after 210/5750 for class 1: 98.3982%\n",
      "Accuracy after 210/5750 for class 2: 95.7816%\n",
      "Accuracy after 210/5750 for class 3: 96.5909%\n",
      "\n",
      "Accuracy after 260/5750 for class 0: 97.7966%\n",
      "Accuracy after 260/5750 for class 1: 98.5213%\n",
      "Accuracy after 260/5750 for class 2: 95.8661%\n",
      "Accuracy after 260/5750 for class 3: 96.3719%\n",
      "\n",
      "Accuracy after 310/5750 for class 0: 98.2019%\n",
      "Accuracy after 310/5750 for class 1: 98.6239%\n",
      "Accuracy after 310/5750 for class 2: 95.3719%\n",
      "Accuracy after 310/5750 for class 3: 96.5863%\n",
      "\n",
      "Accuracy after 360/5750 for class 0: 98.1395%\n",
      "Accuracy after 360/5750 for class 1: 98.7968%\n",
      "Accuracy after 360/5750 for class 2: 95.3488%\n",
      "Accuracy after 360/5750 for class 3: 96.7466%\n",
      "\n",
      "Accuracy after 410/5750 for class 0: 98.1424%\n",
      "Accuracy after 410/5750 for class 1: 98.7342%\n",
      "Accuracy after 410/5750 for class 2: 95.2442%\n",
      "Accuracy after 410/5750 for class 3: 96.6867%\n",
      "\n",
      "Accuracy after 460/5750 for class 0: 98.2375%\n",
      "Accuracy after 460/5750 for class 1: 98.7915%\n",
      "Accuracy after 460/5750 for class 2: 94.8335%\n",
      "Accuracy after 460/5750 for class 3: 96.8835%\n",
      "\n",
      "Accuracy after 510/5750 for class 0: 98.2293%\n",
      "Accuracy after 510/5750 for class 1: 98.7132%\n",
      "Accuracy after 510/5750 for class 2: 94.7692%\n",
      "Accuracy after 510/5750 for class 3: 96.7509%\n",
      "\n",
      "Accuracy after 560/5750 for class 0: 98.1496%\n",
      "Accuracy after 560/5750 for class 1: 98.4962%\n",
      "Accuracy after 560/5750 for class 2: 94.4867%\n",
      "Accuracy after 560/5750 for class 3: 96.5739%\n",
      "\n",
      "Accuracy after 610/5750 for class 0: 98.1599%\n",
      "Accuracy after 610/5750 for class 1: 98.4744%\n",
      "Accuracy after 610/5750 for class 2: 94.4889%\n",
      "Accuracy after 610/5750 for class 3: 96.4113%\n",
      "\n",
      "Accuracy after 660/5750 for class 0: 97.8375%\n",
      "Accuracy after 660/5750 for class 1: 98.5138%\n",
      "Accuracy after 660/5750 for class 2: 94.5500%\n",
      "Accuracy after 660/5750 for class 3: 96.3717%\n",
      "\n",
      "Accuracy after 710/5750 for class 0: 97.8606%\n",
      "Accuracy after 710/5750 for class 1: 98.6111%\n",
      "Accuracy after 710/5750 for class 2: 94.2555%\n",
      "Accuracy after 710/5750 for class 3: 96.5261%\n",
      "\n",
      "Accuracy after 760/5750 for class 0: 97.9440%\n",
      "Accuracy after 760/5750 for class 1: 98.6293%\n",
      "Accuracy after 760/5750 for class 2: 94.4678%\n",
      "Accuracy after 760/5750 for class 3: 96.6821%\n",
      "\n",
      "Accuracy after 810/5750 for class 0: 98.0150%\n",
      "Accuracy after 810/5750 for class 1: 98.6510%\n",
      "Accuracy after 810/5750 for class 2: 94.3174%\n",
      "Accuracy after 810/5750 for class 3: 96.6667%\n",
      "\n",
      "Accuracy after 860/5750 for class 0: 98.0818%\n",
      "Accuracy after 860/5750 for class 1: 98.6165%\n",
      "Accuracy after 860/5750 for class 2: 94.3015%\n",
      "Accuracy after 860/5750 for class 3: 96.7123%\n",
      "\n",
      "Accuracy after 910/5750 for class 0: 98.1818%\n",
      "Accuracy after 910/5750 for class 1: 98.5915%\n",
      "Accuracy after 910/5750 for class 2: 94.4896%\n",
      "Accuracy after 910/5750 for class 3: 96.7721%\n",
      "\n",
      "Accuracy after 960/5750 for class 0: 98.0909%\n",
      "Accuracy after 960/5750 for class 1: 98.6713%\n",
      "Accuracy after 960/5750 for class 2: 94.5235%\n",
      "Accuracy after 960/5750 for class 3: 96.7941%\n",
      "\n",
      "\n",
      "Final accuracy for class 0: 97.9965%\n",
      "Final accuracy for class 1: 98.7389%\n",
      "Final accuracy for class 2: 94.6372%\n",
      "Final accuracy for class 3: 96.7203%\n"
     ]
    }
   ],
   "source": [
    "class_cnt = {}\n",
    "class_correct_cnt = {}\n",
    "for i, batch in enumerate(qd_eval_dataloader):\n",
    "    evalInput(batch, y_true, y_pred)\n",
    "\n",
    "    if i % 50 == 9:\n",
    "        for j, _ in enumerate(classes):\n",
    "            # Intermediate accuracy\n",
    "            int_acc = 0 if j not in class_cnt else class_correct_cnt[j] / class_cnt[j]\n",
    "            \n",
    "            print(\"Accuracy after {}/{} for class {}: {:.4f}%\".format(i + 1, len(qd_eval_dataloader), j, int_acc * 100.))\n",
    "\n",
    "        print()\n",
    "\n",
    "    if i > 1000:\n",
    "        break\n",
    "\n",
    "print()\n",
    "for i, _ in enumerate(classes):\n",
    "    # Intermediate accuracy\n",
    "    int_acc = 0 if i not in class_cnt else class_correct_cnt[i] / class_cnt[i]\n",
    "    \n",
    "    print(\"Final accuracy for class {}: {:.4f}%\".format(i, int_acc * 100.))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364165a1-5144-43fd-920e-55673e177ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
